{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Arabic Sign Language Translation\n",
    "\n",
    "This notebook contains code for running real-time translation from the ArSL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom TemporalAttention Layer\n",
    "\n",
    "This is required for loading the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom temporal attention layer\n",
    "class TemporalAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize attention weight\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='normal')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate attention weights\n",
    "        e = tf.tanh(tf.matmul(x, self.W))\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        # Apply attention to the input sequence\n",
    "        output = x * a\n",
    "        # Aggregate the attentionaly weighted features over the sequence\n",
    "        return tf.reduce_sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display arabic text on frame\n",
    "def display_arabic_text(frame, text):\n",
    "    # Convert OpenCV frame to PIL Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    # Load Arabic font \n",
    "    font = ImageFont.truetype(r\"F:\\SignComm\\model test\\V1\\font\\NotoSansArabic-VariableFont_wdth,wght.ttf\", 30)\n",
    "    \n",
    "    # Reshape and apply Bidi\n",
    "    reshaped_text = reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    \n",
    "    # Draw text\n",
    "    draw.text((50, 100), bidi_text, font=font, fill=(0, 255, 0))\n",
    "    \n",
    "    # Convert back to OpenCV format\n",
    "    frame[:] = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the Arabic label from the index\n",
    "def get_arabic_label(index):\n",
    "    arabic_labels = [ \"اسمك ايه ؟\", \"اشاره\", \"الحمدلله\",\n",
    "                     \"السلام عليكم\",\"الصم\",\"اللغه العربيه\",\"ان شاء الله\",\n",
    "                     \"انا\",\"انت\",\"ايه ؟\",\"برنامج\",\"تخرج\",\n",
    "                     \"جميل\",\"دكتور\",\"شكرا\",\n",
    "                     \"طالب\",\"عامل ايه ؟\",\n",
    "                     \"فكرة\",\"في\",\"كلية حاسبات و معلومات\",\n",
    "                     \"مترجم\",\"مجتمع\",\"مساعده\",\n",
    "                     \"مشروع\",\"ناجح\",\"هدف\",\n",
    "                     \"وعليكم السلام\",\"و\"]\n",
    "    return arabic_labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe Setup for Landmark Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe hand and pose solutions outside the function\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "mp_pose = mp.solutions.pose.Pose(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Function to extract landmarks from a single frame\n",
    "def extract_landmarks_single(frame):\n",
    "    # Use the global MediaPipe hand and pose solutions\n",
    "    global mp_hands, mp_pose\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process landmarks\n",
    "    hand_results = mp_hands.process(frame_rgb)\n",
    "    pose_results = mp_pose.process(frame_rgb)\n",
    "\n",
    "    # Same logic as extract_landmarks but for a single frame\n",
    "    frame_data = []\n",
    "\n",
    "    # Pose landmarks\n",
    "    if pose_results.pose_landmarks:\n",
    "        pose_data = [[lmk.x, lmk.y, lmk.z] for lmk in pose_results.pose_landmarks.landmark]\n",
    "    else:\n",
    "        pose_data = [[0,0,0]]*33\n",
    "\n",
    "    # Hand landmarks\n",
    "    left_hand = [[0,0,0]]*21\n",
    "    right_hand = [[0,0,0]]*21\n",
    "\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
    "                                  hand_results.multi_handedness):\n",
    "            if handedness.classification[0].label == \"Left\":\n",
    "                left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "            else:\n",
    "                right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "\n",
    "    # Flatten and return\n",
    "    frame_data = np.array(pose_data + left_hand + right_hand).flatten()\n",
    "    return frame_data if (np.any(left_hand) or np.any(right_hand)) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for real-time translation\n",
    "def real_time_translation(model, seq_length=30):\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera\n",
    "    buffer = []  # Initialize frame buffer\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()   # Read frame from the camera\n",
    "        if not ret: break   # Break if no frame is read \n",
    "\n",
    "        # Process the frame to get hand and pose landmarks\n",
    "        processed_frame = extract_landmarks_single(frame)\n",
    "\n",
    "        if processed_frame is None:\n",
    "            # Display a text to show hands if not detected\n",
    "            cv2.putText(frame, \"Show Hands\", (50,50),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                      (0,0,255), 2)\n",
    "\n",
    "        # Append the frame to the buffer if landmarks are detected\n",
    "        else:  # Has hands\n",
    "            buffer.append(processed_frame) # Append the processed frame to buffer\n",
    "            buffer = [f for f in buffer if f is not None][-seq_length:]  # Keep only the most recent frames and filter out any None\n",
    "\n",
    "\n",
    "            if len(buffer) == seq_length:\n",
    "                # Make a prediction using the model\n",
    "                prediction = model.predict(np.array([buffer]))\n",
    "                arabic_word = get_arabic_label(np.argmax(prediction)) # Get the predicted word\n",
    "                display_arabic_text(frame, arabic_word) # Display it on the frame\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Translation', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit if 'q' is pressed\n",
    "            break\n",
    "\n",
    "    cap.release()  # Release the camera\n",
    "    cv2.destroyAllWindows()  # Close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Run Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model\n",
    "def load_model(path):\n",
    "    return tf.keras.models.load_model(\n",
    "        path,\n",
    "        custom_objects={'TemporalAttention': TemporalAttention}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run real-time translation\n",
    "# Make sure to update the path to your trained model\n",
    "model = load_model('best_model2.keras')\n",
    "real_time_translation(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
