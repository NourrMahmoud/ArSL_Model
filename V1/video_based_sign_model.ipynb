{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create label mapping based on your dataset's sign names\n",
    "label_mapping = {\n",
    "    'اسمك ايه ؟': 0, 'اشاره': 1, 'الحمدلله': 2,\n",
    "    'السلام عليكم': 3, 'الصم': 4, 'اللغه العربيه': 5,\n",
    "    'ان شاء الله': 6, 'انا': 7, 'انت': 8,\n",
    "    'ايه ؟': 9, 'برنامج': 10, 'تخرج': 11,\n",
    "    'جميل': 12, 'دكتور': 13, 'شكرا': 14,\n",
    "    'طالب': 15, 'عامل ايه ؟': 16, 'فكرة': 17,\n",
    "    'في': 18, 'كلية حاسبات و معلومات': 19, 'مترجم': 20,\n",
    "    'مجتمع': 21, 'مساعده': 22, 'مشروع': 23,\n",
    "    'ناجح': 24, 'هدف': 25, 'وعليكم السلام': 26, 'و': 27\n",
    "}\n",
    "\n",
    "# Create reverse mapping for inference\n",
    "idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_VIDEO_DIR = \"F:/SignComm/model test/V1/Videos\"  # Path to the original videos\n",
    "FRAME_HEIGHT = 128\n",
    "FRAME_WIDTH = 128\n",
    "NUM_FRAMES = 30  # Number of frames to extract from each video\n",
    "NUM_CLASSES = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path, num_frames=30, target_size=(128, 128)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If video has fewer frames than required, we'll duplicate the last frame\n",
    "    if total_frames <= 0:\n",
    "        print(f\"Error reading video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate frame indices to extract (evenly distributed)\n",
    "    if total_frames >= num_frames:\n",
    "        # Extract evenly spaced frames\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    else:\n",
    "        # Use all frames and pad with the last frame\n",
    "        indices = list(range(total_frames)) + [total_frames-1] * (num_frames - total_frames)\n",
    "    \n",
    "    # Extract the frames at calculated indices\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Resize and convert to RGB\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            print(f\"Error reading frame {idx} from {video_path}\")\n",
    "            # Use previous frame if available, or a blank frame\n",
    "            if frames:\n",
    "                frames.append(frames[-1])\n",
    "            else:\n",
    "                frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_dataset(video_dir, label_mapping, num_frames=30, target_size=(128, 128)):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Collect all video paths\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the directory structure\n",
    "    for sign_name in os.listdir(video_dir):\n",
    "        sign_dir = os.path.join(video_dir, sign_name)\n",
    "        if not os.path.isdir(sign_dir):\n",
    "            continue\n",
    "        \n",
    "        # Check if the sign name is in our label mapping\n",
    "        if sign_name not in label_mapping:\n",
    "            print(f\"Warning: Sign '{sign_name}' not found in label mapping, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        label_idx = label_mapping[sign_name]\n",
    "        \n",
    "        # Process all videos for this sign\n",
    "        for signer_name in os.listdir(sign_dir):\n",
    "            signer_dir = os.path.join(sign_dir, signer_name)\n",
    "            if not os.path.isdir(signer_dir):\n",
    "                continue\n",
    "                \n",
    "            for video_file in os.listdir(signer_dir):\n",
    "                if video_file.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "                    video_path = os.path.join(signer_dir, video_file)\n",
    "                    video_paths.append(video_path)\n",
    "                    labels.append(label_idx)\n",
    "    \n",
    "    # Process videos and extract frames\n",
    "    print(f\"Processing {len(video_paths)} videos...\")\n",
    "    \n",
    "    for i, (video_path, label) in enumerate(tqdm(list(zip(video_paths, labels)))):\n",
    "        frames = extract_frames(video_path, num_frames, target_size)\n",
    "        if frames is not None:\n",
    "            X.append(frames)\n",
    "            y.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays and normalize pixel values\n",
    "    X = np.array(X, dtype=np.float32) / 255.0\n",
    "    y = to_categorical(y, num_classes=len(label_mapping))\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into train/test sets\n",
    "def prepare_data():\n",
    "    print(\"Loading and preprocessing dataset...\")\n",
    "    X, y = load_dataset(INPUT_VIDEO_DIR, label_mapping, NUM_FRAMES, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Dataset loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN+LSTM Model\n",
    "def build_cnn_lstm_model(input_shape, num_classes):\n",
    "    # Base CNN model for feature extraction (MobileNetV2 for efficiency)\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(input_shape[1], input_shape[2], 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers to prevent overfitting\n",
    "    for layer in base_model.layers[:-20]:  # Keep some layers trainable\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Sequential([\n",
    "        # The TimeDistributed wrapper applies the CNN to each frame\n",
    "        TimeDistributed(\n",
    "            base_model,\n",
    "            input_shape=(input_shape[0], input_shape[1], input_shape[2], 3)\n",
    "        ),\n",
    "        \n",
    "        # Add LSTM layers for temporal features\n",
    "        LSTM(256, return_sequences=True, dropout=0.3),\n",
    "        LSTM(128, dropout=0.3),\n",
    "        \n",
    "        # Classification layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),  # Lower learning rate for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model():\n",
    "    # Load and prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data()\n",
    "    \n",
    "    # Create the model\n",
    "    input_shape = (NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)\n",
    "    model = build_cnn_lstm_model(input_shape, NUM_CLASSES)\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_cnn_video_model.keras', \n",
    "            monitor='val_accuracy', \n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=8,  # Small batch size due to large model\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display Arabic text on frame\n",
    "def display_arabic_text(frame, text):\n",
    "    # Convert OpenCV frame to PIL Image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    # Load Arabic font\n",
    "    font = ImageFont.truetype(r\"F:\\SignComm\\model test\\V1\\font\\NotoSansArabic-VariableFont_wdth,wght.ttf\", 30)\n",
    "    \n",
    "    # Reshape and apply Bidi\n",
    "    reshaped_text = reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    \n",
    "    # Draw text\n",
    "    draw.text((50, 100), bidi_text, font=font, fill=(0, 255, 0))\n",
    "    \n",
    "    # Convert back to OpenCV format\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time inference function\n",
    "def real_time_inference():\n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model('best_cnn_video_model.keras')\n",
    "    \n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Frame buffer and parameters\n",
    "    frame_buffer = []\n",
    "    buffer_size = NUM_FRAMES\n",
    "    frame_interval = 1  # Capture every N frames\n",
    "    count = 0\n",
    "    \n",
    "    # Inference parameters\n",
    "    prediction_history = []\n",
    "    prediction_window = 5\n",
    "    current_prediction = None\n",
    "    confidence_threshold = 0.7\n",
    "    \n",
    "    print(\"Starting real-time inference. Press 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        # Show the original frame\n",
    "        display_frame = frame.copy()\n",
    "        \n",
    "        # Process frame for the buffer (every N frames)\n",
    "        count += 1\n",
    "        if count % frame_interval == 0:\n",
    "            # Preprocess the frame (resize and normalize)\n",
    "            processed_frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "            processed_frame = processed_frame.astype(np.float32) / 255.0\n",
    "            \n",
    "            # Add to buffer\n",
    "            frame_buffer.append(processed_frame)\n",
    "            \n",
    "            # Keep only the most recent frames\n",
    "            if len(frame_buffer) > buffer_size:\n",
    "                frame_buffer.pop(0)\n",
    "                \n",
    "            # Make prediction when buffer is full\n",
    "            if len(frame_buffer) == buffer_size:\n",
    "                # Prepare input batch [1, num_frames, height, width, channels]\n",
    "                input_data = np.array([frame_buffer])\n",
    "                \n",
    "                # Get prediction\n",
    "                predictions = model.predict(input_data, verbose=0)\n",
    "                pred_idx = np.argmax(predictions[0])\n",
    "                confidence = predictions[0][pred_idx]\n",
    "                \n",
    "                # Display confidence\n",
    "                confidence_text = f\"Confidence: {confidence*100:.1f}%\"\n",
    "                cv2.putText(\n",
    "                    display_frame, confidence_text, (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2\n",
    "                )\n",
    "                \n",
    "                # Add to prediction history if confident enough\n",
    "                if confidence > confidence_threshold:\n",
    "                    prediction_history.append(pred_idx)\n",
    "                    prediction_history = prediction_history[-prediction_window:]  # Keep only recent predictions\n",
    "                    \n",
    "                    # Only update displayed prediction when we have enough history\n",
    "                    if len(prediction_history) >= 3:\n",
    "                        # Get most common prediction in the window\n",
    "                        from collections import Counter\n",
    "                        counter = Counter(prediction_history)\n",
    "                        most_common = counter.most_common(1)[0]\n",
    "                        most_common_idx, count = most_common\n",
    "                        \n",
    "                        # Only change prediction if it appears enough times\n",
    "                        if count >= max(2, len(prediction_history) * 0.6):\n",
    "                            current_prediction = most_common_idx\n",
    "                \n",
    "                # Display the current prediction\n",
    "                if current_prediction is not None:\n",
    "                    arabic_text = idx_to_label[current_prediction]\n",
    "                    display_frame = display_arabic_text(display_frame, arabic_text)\n",
    "        \n",
    "        # Display frame count indicator\n",
    "        buffer_status = f\"Frames: {len(frame_buffer)}/{buffer_size}\"\n",
    "        cv2.putText(\n",
    "            display_frame, buffer_status, (10, 60), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2\n",
    "        )\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow('Real-time Sign Language Recognition', display_frame)\n",
    "        \n",
    "        # Break on 'q' press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution point\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the following line to train the model\n",
    "    # model = train_model()\n",
    "    \n",
    "    # Uncomment the following line to run real-time inference\n",
    "    # real_time_inference()\n",
    "    \n",
    "    print(\"Select an operation by uncommenting the appropriate line in the code.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
