{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install mediapipe tensorflow opencv-python arabic-reshaper python-bidi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adcM378y9keG"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Flatten\n",
        "from arabic_reshaper import reshape\n",
        "from bidi.algorithm import get_display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVTFF0S37n2"
      },
      "source": [
        "1. Preprocessing with MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_video_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/Videos\"\n",
        "output_numpy_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/output_dir\"\n",
        "import os\n",
        "import json\n",
        "# Add this recovery function BEFORE calling process_dataset()\n",
        "def recover_checkpoint(output_dir, checkpoint_file, batch_size):\n",
        "    \"\"\"Rebuild checkpoint from existing output files\"\"\"\n",
        "    # Get all existing output files\n",
        "    existing_files = set([f for f in os.listdir(output_dir) if f.endswith('.npy')])\n",
        "\n",
        "    # Get full list of expected video paths\n",
        "    input_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/Videos\"  # Update this path\n",
        "    video_paths = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov']):\n",
        "                video_path = os.path.join(root, file)\n",
        "                rel_path = os.path.relpath(video_path, input_dir)\n",
        "                output_name = rel_path.replace(os.path.sep, '_').replace(' ', '_') + '.npy'\n",
        "                video_paths.append(output_name)\n",
        "\n",
        "    # Calculate progress\n",
        "    checkpoint = {\n",
        "        'processed': list(existing_files),\n",
        "        'total_videos': len(video_paths),\n",
        "        'version': 2\n",
        "    }\n",
        "\n",
        "    # Calculate batches completed based on processed count and batch size\n",
        "    processed_count = len(existing_files)\n",
        "    checkpoint['batch_counter'] = (processed_count // batch_size)\n",
        "\n",
        "    # Save checkpoint\n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        json.dump(checkpoint, f)\n",
        "\n",
        "    print(f\"Recovered: {processed_count} files | Estimated batches: {checkpoint['batch_counter']//batch_size}\")\n",
        "\n",
        "# Then modify your process_dataset call to:\n",
        "checkpoint_file = '/content/drive/MyDrive/SignComm_Dataset/processing_checkpoint.json'\n",
        "batch_size = 5  # Match your actual batch size\n",
        "\n",
        "# Recover checkpoint before processing\n",
        "if os.path.exists(checkpoint_file):\n",
        "    recover_checkpoint(output_numpy_dir, checkpoint_file, batch_size)\n",
        "\n",
        "# Then run processing as normal\n",
        "# process_dataset(input_video_dir, output_numpy_dir, seq_length=30, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All-in-one cell for running the dataset processing\n",
        "def run_processing():\n",
        "    # 1. Install required packages\n",
        "    import sys\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'mediapipe', 'tensorflow', 'opencv-python',\n",
        "                          'arabic-reshaper', 'python-bidi', 'scikit-learn', 'psutil'])\n",
        "\n",
        "    # 2. Mount Google Drive if in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except ImportError:\n",
        "        print(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "    # 3. Import necessary libraries\n",
        "    import mediapipe as mp\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import json\n",
        "    import gc\n",
        "    import psutil\n",
        "    from pathlib import Path\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # 4. Configure MediaPipe\n",
        "    mp_hands = mp.solutions.hands\n",
        "    mp_pose = mp.solutions.pose\n",
        "\n",
        "    # 5. Memory monitoring function\n",
        "    def print_memory_usage():\n",
        "        process = psutil.Process(os.getpid())\n",
        "        print(f\"Memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "    # 6. Landmark extraction with preallocated arrays\n",
        "    def extract_landmarks(video_path, hands_model, pose_model):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        cap.set(cv2.CAP_PROP_BUFFERSIZE, 10)\n",
        "\n",
        "        num_features = 75 * 3  # 33 pose + 21*2 hands\n",
        "        initial_capacity = 100  # Initial frame buffer size\n",
        "        sequence = np.zeros((initial_capacity, num_features), dtype=np.float32)\n",
        "        current_frame = 0\n",
        "        hands_detected = False\n",
        "\n",
        "        try:\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Process frame\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                hand_results = hands_model.process(frame_rgb)\n",
        "                pose_results = pose_model.process(frame_rgb)\n",
        "\n",
        "                # Initialize landmark data\n",
        "                frame_data = np.zeros(num_features, dtype=np.float32)\n",
        "\n",
        "                # Pose landmarks (0-98)\n",
        "                if pose_results.pose_landmarks:\n",
        "                    pose_flat = np.array(\n",
        "                        [[lmk.x, lmk.y, lmk.z] for lmk in pose_results.pose_landmarks.landmark]\n",
        "                    ).flatten()\n",
        "                    frame_data[:99] = pose_flat\n",
        "\n",
        "                # Hands (99-225)\n",
        "                hand_idx = 99\n",
        "                if hand_results.multi_hand_landmarks:\n",
        "                    for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
        "                                              hand_results.multi_handedness):\n",
        "                        hand_flat = np.array(\n",
        "                            [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "                        ).flatten()\n",
        "                        # Left hand (99-161), Right hand (162-224)\n",
        "                        start_idx = 99 if handedness.classification[0].label == \"Left\" else 162\n",
        "                        frame_data[start_idx:start_idx+63] = hand_flat\n",
        "                        hands_detected = True\n",
        "\n",
        "                # Resize buffer if needed\n",
        "                if current_frame >= sequence.shape[0]:\n",
        "                    new_sequence = np.zeros((sequence.shape[0]*2, num_features), dtype=np.float32)\n",
        "                    new_sequence[:sequence.shape[0]] = sequence\n",
        "                    sequence = new_sequence\n",
        "\n",
        "                sequence[current_frame] = frame_data\n",
        "                current_frame += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame: {str(e)}\")\n",
        "            return None\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n",
        "        if not hands_detected:\n",
        "            print(f\"No hands detected in {video_path}\")\n",
        "            return None\n",
        "\n",
        "        return sequence[:current_frame]\n",
        "\n",
        "    # 7. Dataset processing with batch handling\n",
        "    # Modified dataset processing function with relaxed checkpoint handling\n",
        "    def process_dataset(input_dir, output_dir, seq_length=30, batch_size=10,\n",
        "                      checkpoint_file='processing_checkpoint.json'):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        allowed_extensions = ['.mp4', '.avi', '.mov']\n",
        "\n",
        "        # Initialize checkpoint structure\n",
        "        default_checkpoint = {\n",
        "            'processed': [],\n",
        "            'version': 2\n",
        "        }\n",
        "\n",
        "        # Load or initialize checkpoint\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            try:\n",
        "                with open(checkpoint_file, 'r') as f:\n",
        "                    checkpoint = json.load(f)\n",
        "\n",
        "                # Handle legacy formats\n",
        "                if isinstance(checkpoint, list):  # Old list-only format\n",
        "                    checkpoint = {'processed': checkpoint, 'version': 1}\n",
        "                elif 'total_videos' in checkpoint:  # Remove deprecated field\n",
        "                    del checkpoint['total_videos']\n",
        "\n",
        "                processed_videos = set(checkpoint.get('processed', []))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint: {str(e)}, starting fresh\")\n",
        "                processed_videos = set()\n",
        "        else:\n",
        "            processed_videos = set()\n",
        "\n",
        "        # Always build fresh video list (handles dataset changes)\n",
        "        video_paths = []\n",
        "        for root, _, files in os.walk(input_dir):\n",
        "            for file in files:\n",
        "                if any(file.lower().endswith(ext) for ext in allowed_extensions):\n",
        "                    video_path = os.path.join(root, file)\n",
        "                    rel_path = os.path.relpath(video_path, input_dir)\n",
        "                    output_name = rel_path.replace(os.path.sep, '_').replace(' ', '_') + '.npy'\n",
        "                    video_paths.append((video_path, output_name))\n",
        "\n",
        "        # Filter out already processed videos\n",
        "        unprocessed_videos = [(vp, on) for vp, on in video_paths if on not in processed_videos]\n",
        "        print(f\"Found {len(video_paths)} total videos ({len(unprocessed_videos)} remaining)\")\n",
        "\n",
        "        # Process in batches\n",
        "        for batch_idx in range(0, len(unprocessed_videos), batch_size):\n",
        "            current_batch = unprocessed_videos[batch_idx:batch_idx + batch_size]\n",
        "            print(f\"\\nProcessing batch {(batch_idx//batch_size)+1}/{(len(unprocessed_videos)//batch_size)+1}\")\n",
        "            print_memory_usage()\n",
        "\n",
        "            # Initialize MediaPipe models per batch\n",
        "            hands = mp_hands.Hands()\n",
        "            pose = mp_pose.Pose()\n",
        "            batch_processed = []\n",
        "\n",
        "            try:\n",
        "                for video_path, output_name in current_batch:\n",
        "                    try:\n",
        "                        print(f\"Processing: {os.path.basename(video_path)}\")\n",
        "                        sequence = extract_landmarks(video_path, hands, pose)\n",
        "                        if sequence is None:\n",
        "                            continue\n",
        "\n",
        "                        # Pad/trim sequence\n",
        "                        padded = np.zeros((seq_length, sequence.shape[1]), dtype=np.float32)\n",
        "                        if len(sequence) > seq_length:\n",
        "                            padded = sequence[:seq_length]\n",
        "                        else:\n",
        "                            padded[:len(sequence)] = sequence\n",
        "\n",
        "                        # Save results\n",
        "                        np.save(os.path.join(output_dir, output_name), padded)\n",
        "                        batch_processed.append(output_name)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {video_path}: {str(e)}\")\n",
        "\n",
        "                # Update checkpoint after batch\n",
        "                processed_videos.update(batch_processed)\n",
        "                with open(checkpoint_file, 'w') as f:\n",
        "                    json.dump({\n",
        "                        'processed': list(processed_videos),\n",
        "                        'version': 2\n",
        "                    }, f)\n",
        "                print(f\"Completed batch {(batch_idx//batch_size)+1}\")\n",
        "                print_memory_usage()\n",
        "\n",
        "            finally:\n",
        "                # Release resources\n",
        "                hands.close()\n",
        "                pose.close()\n",
        "                del hands\n",
        "                del pose\n",
        "                gc.collect()\n",
        "\n",
        "        print(f\"\\nProcessing completed. Total videos processed: {len(processed_videos)}\")\n",
        "        return processed_videos\n",
        "\n",
        "    # 8. Set paths and run processing\n",
        "    input_video_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/Videos\"\n",
        "    output_numpy_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/output_dir\"\n",
        "\n",
        "    return process_dataset(input_video_dir, output_numpy_dir, seq_length=30, batch_size=5, checkpoint_file=\"/content/drive/MyDrive/SignComm_Dataset/processing_checkpoint.json\")\n",
        "\n",
        "# Execute the function\n",
        "processed_videos = run_processing()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# input_video_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/Videos\"\n",
        "# output_numpy_dir = \"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/output_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWRv5_-37n3"
      },
      "source": [
        "2. Data Conversion to Numpy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to process all videos in a directory and save as numpy arrays\n",
        "def process_dataset(input_dir, output_dir, seq_length=30, batch_size=10, checkpoint_file='processing_checkpoint.json'):\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  allowed_extensions = ['.mp4', '.avi', '.mov']\n",
        "  num_features = 75 * 3  # 33 pose + 21*2 hands\n",
        "\n",
        "  # Load checkpoint if exists\n",
        "  processed_videos = set()\n",
        "  if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "      processed_videos = set(json.load(f))\n",
        "    print(f\"Loaded checkpoint with {len(processed_videos)} processed videos\")\n",
        "\n",
        "  # Get all video paths first\n",
        "  all_videos = []\n",
        "  for sign_name in os.listdir(input_dir):\n",
        "    sign_path = os.path.join(input_dir, sign_name)\n",
        "    if not os.path.isdir(sign_path):\n",
        "      continue\n",
        "\n",
        "    for signer_name in os.listdir(sign_path):\n",
        "      signer_path = os.path.join(sign_path, signer_name)\n",
        "      if not os.path.isdir(signer_path):\n",
        "        continue\n",
        "\n",
        "      for video_file in os.listdir(signer_path):\n",
        "        if not any(video_file.lower().endswith(ext) for ext in allowed_extensions):\n",
        "          continue\n",
        "\n",
        "        video_path = os.path.join(signer_path, video_file)\n",
        "        output_name = f\"{sign_name}_{signer_name}_{os.path.splitext(video_file)[0]}.npy\"\n",
        "        output_path = os.path.join(output_dir, output_name)\n",
        "\n",
        "        # Skip if already processed\n",
        "        if os.path.exists(output_path) or output_name in processed_videos:\n",
        "          continue\n",
        "\n",
        "        all_videos.append((video_path, sign_name, signer_name, video_file))\n",
        "\n",
        "  # Process in batches to limit memory usage\n",
        "  total_videos = len(all_videos)\n",
        "  print(f\"Found {total_videos} videos to process\")\n",
        "\n",
        "  # Process in batches to limit memory usage\n",
        "  for i in range(0, total_videos, batch_size):\n",
        "    batch = all_videos[i:i+batch_size]\n",
        "    print(f\"Processing batch {i//batch_size + 1}/{(total_videos + batch_size - 1)//batch_size}: {i}-{min(i+batch_size, total_videos)} of {total_videos} videos\")\n",
        "\n",
        "    for video_index, (video_path, sign_name, signer_name, video_file) in enumerate(batch):\n",
        "      try:\n",
        "        print(f\"  Processing video {i + video_index + 1}/{total_videos}: {video_file}\")\n",
        "\n",
        "        # Extract landmarks\n",
        "        sequence = extract_landmarks(video_path)\n",
        "\n",
        "        if len(sequence) == 0:\n",
        "          print(f\"  Skipping {video_path} - no hands detected\")\n",
        "          continue\n",
        "\n",
        "        # Create padded sequence\n",
        "        padded_sequence = np.zeros((seq_length, num_features))\n",
        "\n",
        "        if len(sequence) > seq_length:\n",
        "          padded_sequence = sequence[:seq_length]\n",
        "        else:\n",
        "          padded_sequence[:len(sequence)] = sequence\n",
        "\n",
        "        # Save numpy array\n",
        "        output_name = f\"{sign_name}_{signer_name}_{os.path.splitext(video_file)[0]}.npy\"\n",
        "        np.save(os.path.join(output_dir, output_name), padded_sequence)\n",
        "\n",
        "        # Add to processed videos and update checkpoint\n",
        "        processed_videos.add(output_name)\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "          json.dump(list(processed_videos), f)\n",
        "\n",
        "        # Force garbage collection after each video\n",
        "        sequence = None\n",
        "        padded_sequence = None\n",
        "        gc.collect()\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"  Error processing {video_path}: {str(e)}\")\n",
        "\n",
        "      # Explicitly release MediaPipe resources\n",
        "      if 'mp_hands' in locals():\n",
        "        mp_hands.close()\n",
        "      if 'mp_pose' in locals():\n",
        "        mp_pose.close()\n",
        "\n",
        "      # Force garbage collection after each video\n",
        "      gc.collect()\n",
        "\n",
        "    # Force aggressive garbage collection after each batch\n",
        "    gc.collect()\n",
        "\n",
        "  print(f\"Processing completed. Total videos processed: {len(processed_videos)}\")\n",
        "  return processed_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the paths\n",
        "input_video_dir = \"/path/to/your/raw/videos\"  # Raw videos (28 subfolders)\n",
        "output_numpy_dir = \"/path/to/processed/data\"  # Processed numpy data\n",
        "\n",
        "# to process all videos\n",
        "process_dataset(input_video_dir, output_numpy_dir, seq_length=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------END OF PREPROCESSING AND DATA CONVERSION TO NUMPY----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOlaw2uPIO2"
      },
      "source": [
        "3. Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def find_full_key_by_first_word(first_word):\n",
        "    for key in label_mapping.keys():\n",
        "        # Split the key into words and check if the first word matches the input\n",
        "        words = key.split()\n",
        "        if words and words[0] == first_word:\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "# Function to load data from numpy arrays and prepare for model training\n",
        "def load_data(numpy_dir, label_mapping):\n",
        "    X = []  # List to hold feature data\n",
        "    y = []  # List to hold label data\n",
        "\n",
        "    # Loop through each numpy file in directory\n",
        "    for file in os.listdir(numpy_dir):\n",
        "        if not file.endswith(\".npy\") or file.startswith(\"اسمك\") :  # Skip non-numpy files\n",
        "            continue\n",
        "\n",
        "        # Extract label from the first part of the filename\n",
        "        label = file.split(\"_\")[0]\n",
        "        label = find_full_key_by_first_word(label)\n",
        "        class_idx = label_mapping[label]  # Get the class index from label mapping\n",
        "\n",
        "        data = np.load(os.path.join(numpy_dir, file)) # Load numpy array from the file\n",
        "        X.append(data)  # Append data to features list\n",
        "        y.append(class_idx) # Append label to labels list\n",
        "\n",
        "    X = np.array(X)  # Convert feature list to numpy array\n",
        "    y = to_categorical(y, num_classes=28)  # Convert labels to encoded format\n",
        "    return train_test_split(X, y, test_size=0.2) # Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktNi97CS37n3"
      },
      "source": [
        "4. LSTM Model with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZT57gZPIO2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer, LSTM, Dense, Permute, Multiply, Flatten\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "# Define custom temporal attention layer\n",
        "class TemporalAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize attention weight\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='normal')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Calculate attention weights\n",
        "        e = tf.tanh(tf.matmul(x, self.W))\n",
        "        a = tf.nn.softmax(e, axis=1)\n",
        "        # Apply attention to the input sequence\n",
        "        output = x * a\n",
        "        # Aggregate the attentionaly weighted features over the sequence\n",
        "        return tf.reduce_sum(output, axis=1)\n",
        "\n",
        "# Function to build the LSTM model with attention\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Bi-directional LSTM layers with return sequences, Combat overfitting by adding dropout layers to the LSTM model\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(inputs)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(x)\n",
        "\n",
        "    # Temporal Attention Layer\n",
        "    attention = TemporalAttention()(x)\n",
        "\n",
        "    # Classification using dense layer\n",
        "    outputs = Dense(num_classes, activation='softmax')(attention)\n",
        "\n",
        "    # Create model object\n",
        "    model = Model(inputs, outputs)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052pHi3n37n4"
      },
      "source": [
        "5. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe_d1acE37n4"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate the model\n",
        "def train_model(X_train, y_train, X_test, y_test):\n",
        "    # Build the model using the specified input shape and number of classes\n",
        "    model = build_model(X_train.shape[1:], 28)\n",
        "\n",
        "    # Define callbacks for early stopping and saving the best model\n",
        "    callbacks = [\n",
        "        # tf.keras.callbacks.EarlyStopping(patience=10),\n",
        "        tf.keras.callbacks.ModelCheckpoint('best_model2.keras', save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train,\n",
        "                      validation_data=(X_test, y_test),\n",
        "                      epochs=100,\n",
        "                      batch_size=32,\n",
        "                      verbose=1,  # show progress\n",
        "                      callbacks=callbacks)\n",
        "    return model   # Return the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWm6NolVPIO2"
      },
      "outputs": [],
      "source": [
        "# Create label mapping based on your dataset's sign names\n",
        "label_mapping = {\n",
        "    'اسمك ايه ؟': 0,\n",
        "    'اشاره': 1,\n",
        "    'الحمدلله': 2,\n",
        "    'السلام عليكم': 3,\n",
        "    'اللغه العربيه': 4,\n",
        "    'ان شاء الله': 5,\n",
        "    'انا': 6,\n",
        "    'انت': 7,\n",
        "    'ايه ؟': 8,\n",
        "    'برنامج': 9,\n",
        "    'تخرج': 10,\n",
        "    'جميل': 11,\n",
        "    'دكتور': 12,\n",
        "    'شكرا': 13,\n",
        "    'الصم': 14,\n",
        "    'طالب': 15,\n",
        "    'عامل ايه ؟': 16,\n",
        "    'فكرة': 17,\n",
        "    'في': 18,\n",
        "    'كلية حاسبات و معلومات': 19,\n",
        "    'مترجم': 20,\n",
        "    'مجتمع': 21,\n",
        "    'مساعده': 22,\n",
        "    'مشروع': 23,\n",
        "    'ناجح': 24,\n",
        "    'هدف': 25,\n",
        "    'وعليكم السلام': 26,\n",
        "    'و': 27,\n",
        "}\n",
        "\n",
        "output_numpy_dir = r\"C:\\Users\\DELL\\Desktop\\ArSL_Model2\\processed_npy_arrays\"  # Processed numpy data\n",
        "\n",
        "# Load data and train\n",
        "X_train, X_test, y_train, y_test = load_data(output_numpy_dir, label_mapping)\n",
        "model = train_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Final evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Final Model Accuracy: {accuracy*100:.2f}%\")  #accuracy 99.02%, in 31m and 30.6s execution time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------END OF MODEL TRAINING----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVsH0wlDPIO2"
      },
      "source": [
        "6. Arabic Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "from arabic_reshaper import reshape\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "# # Function to display arabic text on frame\n",
        "def display_arabic_text(frame, text):\n",
        "    # Convert OpenCV frame to PIL Image\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    draw = ImageDraw.Draw(pil_image)\n",
        "    \n",
        "    # Load Arabic font \n",
        "    font = ImageFont.truetype(r\"C:\\Users\\DELL\\Desktop\\ArSL_Model2\\font\\static\\NotoSansArabic_Condensed-Bold.ttf\", 30)\n",
        "    \n",
        "    # Reshape and apply Bidi\n",
        "    reshaped_text = reshape(text)\n",
        "    bidi_text = get_display(reshaped_text)\n",
        "    \n",
        "    # Draw text\n",
        "    draw.text((50, 100), bidi_text, font=font, fill=(0, 255, 0))\n",
        "    \n",
        "    # Convert back to OpenCV format\n",
        "    frame[:] = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROJSevnFPIO3"
      },
      "outputs": [],
      "source": [
        "# Function to get the Arabic label from the index\n",
        "def get_arabic_label(index):\n",
        "    arabic_labels = [ \"اسمك ايه ؟\", \"اشاره\", \"الحمدلله\",\n",
        "                     \"السلام عليكم\",\"الصم\",\"اللغه العربيه\",\"ان شاء الله\",\n",
        "                     \"انا\",\"انت\",\"ايه ؟\",\"برنامج\",\"تخرج\",\n",
        "                     \"جميل\",\"دكتور\",\"شكرا\",\n",
        "                     \"طالب\",\"عامل ايه ؟\",\n",
        "                     \"فكرة\",\"في\",\"كلية حاسبات و معلومات\",\n",
        "                     \"مترجم\",\"مجتمع\",\"مساعده\",\n",
        "                     \"مشروع\",\"ناجح\",\"هدف\",\n",
        "                     \"وعليكم السلام\",\"و\"]\n",
        "    return arabic_labels[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MediaPipe hand and pose solutions outside the function\n",
        "mp_hands = mp.solutions.hands.Hands(\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.7\n",
        ")\n",
        "mp_pose = mp.solutions.pose.Pose(\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.7\n",
        ")\n",
        "\n",
        "# Function to extract landmarks from a single frame\n",
        "def extract_landmarks_single(frame):\n",
        "    # Use the global MediaPipe hand and pose solutions\n",
        "    global mp_hands, mp_pose\n",
        "\n",
        "    # Convert frame to RGB for MediaPipe\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process landmarks\n",
        "    hand_results = mp_hands.process(frame_rgb)\n",
        "    pose_results = mp_pose.process(frame_rgb)\n",
        "\n",
        "    # Same logic as extract_landmarks but for a single frame\n",
        "    frame_data = []\n",
        "\n",
        "    # Pose landmarks\n",
        "    if pose_results.pose_landmarks:\n",
        "        pose_data = [[lmk.x, lmk.y, lmk.z] for lmk in pose_results.pose_landmarks.landmark]\n",
        "    else:\n",
        "        pose_data = [[0,0,0]]*33\n",
        "\n",
        "    # Hand landmarks\n",
        "    left_hand = [[0,0,0]]*21\n",
        "    right_hand = [[0,0,0]]*21\n",
        "\n",
        "    if hand_results.multi_hand_landmarks:\n",
        "        for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
        "                                  hand_results.multi_handedness):\n",
        "            if handedness.classification[0].label == \"Left\":\n",
        "                left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "            else:\n",
        "                right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "\n",
        "    # Flatten and return\n",
        "    frame_data = np.array(pose_data + left_hand + right_hand).flatten()\n",
        "    return frame_data if (np.any(left_hand) or np.any(right_hand)) else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYYijCcz37n4"
      },
      "source": [
        "7. Real-Time Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVFngfPH37n4"
      },
      "outputs": [],
      "source": [
        "# Function for real-time translation\n",
        "def real_time_translation(model, seq_length=30):\n",
        "    cap = cv2.VideoCapture(0)  # Open default camera\n",
        "    buffer = []  # Initialize frame buffer\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()   # Read frame from the camera\n",
        "        if not ret: break   # Break if no frame is read \n",
        "\n",
        "        # Process the frame to get hand and pose landmarks\n",
        "        processed_frame = extract_landmarks_single(frame)\n",
        "\n",
        "        if processed_frame is None:\n",
        "            # Display a text to show hands if not detected\n",
        "            cv2.putText(frame, \"Show Hands\", (50,50),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                      (0,0,255), 2)\n",
        "\n",
        "        # Append the frame to the buffer if landmarks are detected\n",
        "        else:  # Has hands\n",
        "            buffer.append(processed_frame) # Append the processed frame to buffer\n",
        "            buffer = [f for f in buffer if f is not None][-seq_length:]  # Keep only the most recent frames and filter out any None\n",
        "\n",
        "\n",
        "            if len(buffer) == seq_length:\n",
        "                # Make a prediction using the model\n",
        "                prediction = model.predict(np.array([buffer]))\n",
        "                arabic_word = get_arabic_label(np.argmax(prediction)) # Get the predicted word\n",
        "                display_arabic_text(frame, arabic_word) # Display it on the frame\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow('Translation', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit if 'q' is pressed\n",
        "            break\n",
        "\n",
        "    cap.release()  # Release the camera\n",
        "    cv2.destroyAllWindows()  # Close all windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ayz8s237n4"
      },
      "source": [
        "8. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Uo1_ag37n4"
      },
      "outputs": [],
      "source": [
        "# Function to load the model\n",
        "def load_model(path):\n",
        "    return tf.keras.models.load_model(\n",
        "        path,\n",
        "        custom_objects={'TemporalAttention': TemporalAttention}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxdfI4i_PIO3"
      },
      "outputs": [],
      "source": [
        "model = load_model('best_model2.keras')\n",
        "real_time_translation(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
