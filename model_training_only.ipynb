{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adcM378y9keG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Flatten\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qVTFF0S37n2"
   },
   "source": [
    "1. Preprocessing with MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQc89NAK0g0k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to extract pose and hand landmarks from a video\n",
    "def extract_landmarks(video_path):\n",
    "    # Initialize MediaPipe solutions for hands and pose detection\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "    hands = mp_hands.Hands()\n",
    "    pose = mp_pose.Pose()\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    sequence = []   # List to hold frame data\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break    # Break if no frame\n",
    "\n",
    "        # Process frame\n",
    "        # Convert frame to RGB for MediaPipe processing\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to get hand and pose landmarks\n",
    "        hand_results = hands.process(frame_rgb)\n",
    "        pose_results = pose.process(frame_rgb)\n",
    "\n",
    "\n",
    "        # Get landmarks, Initialize lists to hold landmarks\n",
    "        frame_data = []\n",
    "\n",
    "        # Extract pose landmarks (33 landmarks)\n",
    "        if pose_results.pose_landmarks:\n",
    "            pose_data = [[lmk.x, lmk.y, lmk.z] for lmk in pose_results.pose_landmarks.landmark]\n",
    "        else:\n",
    "            pose_data = [[0,0,0]]*33   # Pad if no pose detected\n",
    "\n",
    "        # Initialize lists for hands landmarks (21 landmarks each)\n",
    "        left_hand = [[0,0,0]]*21\n",
    "        right_hand = [[0,0,0]]*21\n",
    "\n",
    "        # Extract hand landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
    "                                      hand_results.multi_handedness):\n",
    "                if handedness.classification[0].label == \"Left\":\n",
    "                    left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "                else:\n",
    "                    right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "\n",
    "        # Flatten and combine all landmarks\n",
    "        frame_data = np.array(pose_data + left_hand + right_hand).flatten()\n",
    "\n",
    "        # Only add frames with hands (Append frame to sequence only if hands are present)\n",
    "        if (np.any(left_hand != [0,0,0]) or np.any(right_hand != [0,0,0])):\n",
    "           sequence.append(frame_data)\n",
    "\n",
    "    if len(sequence) == 0:\n",
    "        print(f\"Warning: No hands detected in {video_path}\")\n",
    "    return np.array(sequence)\n",
    "\n",
    "    cap.release()  # Release the video\n",
    "    return np.array(sequence)  # Return the sequence of landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK58sFGXPIO1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe hand and pose solutions outside the function\n",
    "mp_hands = mp.solutions.hands.Hands()\n",
    "mp_pose = mp.solutions.pose.Pose()\n",
    "\n",
    "# Function to extract landmarks from a single frame\n",
    "def extract_landmarks_single(frame):\n",
    "    # Use the global MediaPipe hand and pose solutions\n",
    "    global mp_hands, mp_pose\n",
    "    hands = mp_hands.Hands()\n",
    "    pose = mp_pose.Pose()\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process landmarks\n",
    "    hand_results = hands.process(frame_rgb)\n",
    "    pose_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Same logic as extract_landmarks but for a single frame\n",
    "    frame_data = []\n",
    "\n",
    "    # Pose landmarks\n",
    "    if pose_results.pose_landmarks:\n",
    "        pose_data = [[lmk.x, lmk.y, lmk.z] for lmk in pose_results.pose_landmarks.landmark]\n",
    "    else:\n",
    "        pose_data = [[0,0,0]]*33\n",
    "\n",
    "    # Hand landmarks\n",
    "    left_hand = [[0,0,0]]*21\n",
    "    right_hand = [[0,0,0]]*21\n",
    "\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
    "                                  hand_results.multi_handedness):\n",
    "            if handedness.classification[0].label == \"Left\":\n",
    "                left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "            else:\n",
    "                right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
    "\n",
    "    # Flatten and return\n",
    "    frame_data = np.array(pose_data + left_hand + right_hand).flatten()\n",
    "    return frame_data if (np.any(left_hand) or np.any(right_hand)) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CWRv5_-37n3"
   },
   "source": [
    "2. Data Conversion to Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cQWwgdU37n3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to process all videos in a directory and save as numpy arrays\n",
    "def process_dataset(input_dir, output_dir, seq_length=30):\n",
    "    os.makedirs(output_dir, exist_ok=True) # Create output directory if it doesn't exist\n",
    "    allowed_extensions = ['.mp4', '.avi', '.mov']  # List of video file extensions\n",
    "\n",
    "    # Loop through each sign folder\n",
    "    for sign_name in os.listdir(input_dir):\n",
    "        sign_path = os.path.join(input_dir, sign_name)\n",
    "        if not os.path.isdir(sign_path):  # Skip if it's not a directory\n",
    "            continue\n",
    "\n",
    "        # Loop through each signer folder\n",
    "        for signer_name in os.listdir(sign_path):\n",
    "            signer_path = os.path.join(sign_path, signer_name)\n",
    "            if not os.path.isdir(signer_path):\n",
    "                continue\n",
    "\n",
    "            # Loop through each video file\n",
    "            for video_file in os.listdir(signer_path):\n",
    "                # Skip non-video files\n",
    "                if not any(video_file.lower().endswith(ext) for ext in allowed_extensions):\n",
    "                  continue\n",
    "\n",
    "                video_path = os.path.join(signer_path, video_file)\n",
    "\n",
    "                sequence = extract_landmarks(video_path)  # Extract landmarks from video\n",
    "\n",
    "                if len(sequence) == 0:  # Skip videos with no hands detected\n",
    "                  continue\n",
    "\n",
    "                # Padding with zeros to match sequence length\n",
    "                num_features = 75 * 3  # 33 pose + 21*2 hands\n",
    "                padded_sequence = np.zeros((seq_length, num_features))\n",
    "\n",
    "                if len(sequence) > seq_length:\n",
    "                   padded_sequence = sequence[:seq_length]  # If sequence is longer, truncate\n",
    "                else:\n",
    "                   padded_sequence[:len(sequence)] = sequence   # If sequence is shorter, pad with zeros\n",
    "\n",
    "                # Save with sign_name in filename\n",
    "                base_name = os.path.splitext(video_file)[0]\n",
    "                numpy_filename = f\"{sign_name}_{signer_name}_{base_name}.npy\"\n",
    "                np.save(os.path.join(output_dir, numpy_filename), padded_sequence) # Save the padded sequence as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fennVKM4uAaC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "input_video_dir = r\"C:\\Users\\DELL\\Desktop\\ArSL_Model\\ArSL_Dataset\\Videos\"  # Raw videos (28 subfolders)\n",
    "output_numpy_dir = r\"C:\\Users\\DELL\\Desktop\\ArSL_Model\\output_dir\"  # Processed numpy data\n",
    "model_save_path = r\"C:\\Users\\DELL\\Desktop\\ArSL_Model\\model.h5\"     # Trained model path\n",
    "\n",
    "# to process all videos\n",
    "process_dataset(input_video_dir, output_numpy_dir, seq_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDOlaw2uPIO2"
   },
   "source": [
    "3. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:26:37.200283Z",
     "iopub.status.busy": "2025-03-02T12:26:37.199902Z",
     "iopub.status.idle": "2025-03-02T12:26:55.185615Z",
     "shell.execute_reply": "2025-03-02T12:26:55.184903Z",
     "shell.execute_reply.started": "2025-03-02T12:26:37.200245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Flatten\n",
    "# from arabic_reshaper import reshape\n",
    "# from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:26:55.187171Z",
     "iopub.status.busy": "2025-03-02T12:26:55.186635Z",
     "iopub.status.idle": "2025-03-02T12:26:55.193318Z",
     "shell.execute_reply": "2025-03-02T12:26:55.192430Z",
     "shell.execute_reply.started": "2025-03-02T12:26:55.187146Z"
    },
    "id": "8mtV0Z6cPIO2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_full_key_by_first_word(first_word):\n",
    "    for key in label_mapping.keys():\n",
    "        # Split the key into words and check if the first word matches the input\n",
    "        words = key.split()\n",
    "        if words and words[0] == first_word:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "# Function to load data from numpy arrays and prepare for model training\n",
    "def load_data(numpy_dir, label_mapping):\n",
    "    X = []  # List to hold feature data\n",
    "    y = []  # List to hold label data\n",
    "\n",
    "    # Loop through each numpy file in directory\n",
    "    for file in os.listdir(numpy_dir):\n",
    "        if not file.endswith(\".npy\") or file.startswith(\"اسمك\") :  # Skip non-numpy files\n",
    "            continue\n",
    "\n",
    "        # Extract label from the first part of the filename\n",
    "        label = file.split(\"_\")[0]\n",
    "        label = find_full_key_by_first_word(label)\n",
    "        class_idx = label_mapping[label]  # Get the class index from label mapping\n",
    "\n",
    "        data = np.load(os.path.join(numpy_dir, file)) # Load numpy array from the file\n",
    "        X.append(data)  # Append data to features list\n",
    "        y.append(class_idx) # Append label to labels list\n",
    "\n",
    "    X = np.array(X)  # Convert feature list to numpy array\n",
    "    y = to_categorical(y, num_classes=28)  # Convert labels to encoded format\n",
    "    return train_test_split(X, y, test_size=0.2) # Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktNi97CS37n3"
   },
   "source": [
    "4. LSTM Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:26:55.645597Z",
     "iopub.status.busy": "2025-03-02T12:26:55.645257Z",
     "iopub.status.idle": "2025-03-02T12:26:55.652553Z",
     "shell.execute_reply": "2025-03-02T12:26:55.651503Z",
     "shell.execute_reply.started": "2025-03-02T12:26:55.645572Z"
    },
    "id": "sYZT57gZPIO2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Permute, Multiply, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Define custom temporal attention layer\n",
    "class TemporalAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize attention weight\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='normal')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate attention weights\n",
    "        e = tf.tanh(tf.matmul(x, self.W))\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        # Apply attention to the input sequence\n",
    "        output = x * a\n",
    "        # Aggregate the attentionaly weighted features over the sequence\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Function to build the LSTM model with attention\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Bi-directional LSTM layers with return sequences\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(inputs)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "\n",
    "    # Temporal Attention Layer\n",
    "    attention = TemporalAttention()(x)\n",
    "\n",
    "    # Classification using dense layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(attention)\n",
    "\n",
    "    # Create model object\n",
    "    model = Model(inputs, outputs)\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "# print(\"ran\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "052pHi3n37n4"
   },
   "source": [
    "5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:41:34.923352Z",
     "iopub.status.busy": "2025-03-02T12:41:34.923010Z",
     "iopub.status.idle": "2025-03-02T12:41:34.928110Z",
     "shell.execute_reply": "2025-03-02T12:41:34.927055Z",
     "shell.execute_reply.started": "2025-03-02T12:41:34.923329Z"
    },
    "id": "fe_d1acE37n4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    # Build the model using the specified input shape and number of classes\n",
    "    model = build_model(X_train.shape[1:], 28)\n",
    "\n",
    "    # Define callbacks for early stopping and saving the best model\n",
    "    callbacks = [\n",
    "        # I'll remove EarlyStopping to ensure full 5 epochs\n",
    "        # tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      epochs=100,\n",
    "                      batch_size=64,\n",
    "                      verbose=1,  # show progress\n",
    "                      callbacks=callbacks)\n",
    "    return model   # Return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:41:38.114972Z",
     "iopub.status.busy": "2025-03-02T12:41:38.114667Z",
     "iopub.status.idle": "2025-03-02T12:49:44.159710Z",
     "shell.execute_reply": "2025-03-02T12:49:44.158926Z",
     "shell.execute_reply.started": "2025-03-02T12:41:38.114946Z"
    },
    "id": "RWm6NolVPIO2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4668 - loss: 1.6701 - val_accuracy: 0.8540 - val_loss: 0.4562\n",
      "Epoch 2/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8612 - loss: 0.4335 - val_accuracy: 0.9221 - val_loss: 0.2692\n",
      "Epoch 3/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9062 - loss: 0.2911 - val_accuracy: 0.9312 - val_loss: 0.2228\n",
      "Epoch 4/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9378 - loss: 0.1923 - val_accuracy: 0.9403 - val_loss: 0.1891\n",
      "Epoch 5/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9565 - loss: 0.1426 - val_accuracy: 0.9474 - val_loss: 0.1602\n",
      "Epoch 6/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9592 - loss: 0.1228 - val_accuracy: 0.9661 - val_loss: 0.1128\n",
      "Epoch 7/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9681 - loss: 0.0953 - val_accuracy: 0.9630 - val_loss: 0.1125\n",
      "Epoch 8/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9670 - loss: 0.0957 - val_accuracy: 0.9718 - val_loss: 0.0853\n",
      "Epoch 9/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9754 - loss: 0.0708 - val_accuracy: 0.9732 - val_loss: 0.0825\n",
      "Epoch 10/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9778 - loss: 0.0688 - val_accuracy: 0.9790 - val_loss: 0.0685\n",
      "Epoch 11/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9801 - loss: 0.0667 - val_accuracy: 0.9692 - val_loss: 0.0943\n",
      "Epoch 12/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9700 - loss: 0.0916 - val_accuracy: 0.9823 - val_loss: 0.0607\n",
      "Epoch 13/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9816 - loss: 0.0583 - val_accuracy: 0.9773 - val_loss: 0.0683\n",
      "Epoch 14/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9877 - loss: 0.0422 - val_accuracy: 0.9816 - val_loss: 0.0600\n",
      "Epoch 15/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0525 - val_accuracy: 0.9771 - val_loss: 0.0753\n",
      "Epoch 16/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9810 - loss: 0.0606 - val_accuracy: 0.9830 - val_loss: 0.0516\n",
      "Epoch 17/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0540 - val_accuracy: 0.9728 - val_loss: 0.0794\n",
      "Epoch 18/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9854 - loss: 0.0447 - val_accuracy: 0.9508 - val_loss: 0.1518\n",
      "Epoch 19/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9803 - loss: 0.0606 - val_accuracy: 0.9869 - val_loss: 0.0473\n",
      "Epoch 20/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9939 - loss: 0.0218 - val_accuracy: 0.9857 - val_loss: 0.0480\n",
      "Epoch 21/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9917 - loss: 0.0264 - val_accuracy: 0.9842 - val_loss: 0.0538\n",
      "Epoch 22/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9927 - loss: 0.0227 - val_accuracy: 0.9900 - val_loss: 0.0383\n",
      "Epoch 23/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9935 - loss: 0.0206 - val_accuracy: 0.9866 - val_loss: 0.0463\n",
      "Epoch 24/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9908 - loss: 0.0253 - val_accuracy: 0.9804 - val_loss: 0.0577\n",
      "Epoch 25/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9912 - loss: 0.0263 - val_accuracy: 0.9835 - val_loss: 0.0514\n",
      "Epoch 26/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9899 - loss: 0.0309 - val_accuracy: 0.9871 - val_loss: 0.0464\n",
      "Epoch 27/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9878 - loss: 0.0406 - val_accuracy: 0.9694 - val_loss: 0.0925\n",
      "Epoch 28/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9843 - loss: 0.0467 - val_accuracy: 0.9854 - val_loss: 0.0505\n",
      "Epoch 29/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9846 - loss: 0.0475 - val_accuracy: 0.9708 - val_loss: 0.0926\n",
      "Epoch 30/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9905 - loss: 0.0357 - val_accuracy: 0.9895 - val_loss: 0.0330\n",
      "Epoch 31/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9942 - loss: 0.0174 - val_accuracy: 0.9900 - val_loss: 0.0322\n",
      "Epoch 32/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0097 - val_accuracy: 0.9854 - val_loss: 0.0406\n",
      "Epoch 33/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9944 - loss: 0.0181 - val_accuracy: 0.9900 - val_loss: 0.0329\n",
      "Epoch 34/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9946 - loss: 0.0157 - val_accuracy: 0.9866 - val_loss: 0.0454\n",
      "Epoch 35/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9923 - loss: 0.0228 - val_accuracy: 0.9864 - val_loss: 0.0478\n",
      "Epoch 36/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9916 - loss: 0.0264 - val_accuracy: 0.9849 - val_loss: 0.0473\n",
      "Epoch 37/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9858 - loss: 0.0416 - val_accuracy: 0.9852 - val_loss: 0.0470\n",
      "Epoch 38/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9958 - loss: 0.0125 - val_accuracy: 0.9912 - val_loss: 0.0289\n",
      "Epoch 39/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0066 - val_accuracy: 0.9866 - val_loss: 0.0415\n",
      "Epoch 40/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9946 - loss: 0.0174 - val_accuracy: 0.9814 - val_loss: 0.0529\n",
      "Epoch 41/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9942 - loss: 0.0183 - val_accuracy: 0.9847 - val_loss: 0.0524\n",
      "Epoch 42/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9908 - loss: 0.0257 - val_accuracy: 0.9876 - val_loss: 0.0416\n",
      "Epoch 43/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9944 - loss: 0.0185 - val_accuracy: 0.9907 - val_loss: 0.0260\n",
      "Epoch 44/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9976 - loss: 0.0066 - val_accuracy: 0.9857 - val_loss: 0.0459\n",
      "Epoch 45/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9951 - loss: 0.0142 - val_accuracy: 0.9840 - val_loss: 0.0509\n",
      "Epoch 46/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9960 - loss: 0.0133 - val_accuracy: 0.9904 - val_loss: 0.0356\n",
      "Epoch 47/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9962 - loss: 0.0105 - val_accuracy: 0.9931 - val_loss: 0.0245\n",
      "Epoch 48/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 0.9575 - val_loss: 0.1244\n",
      "Epoch 49/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9841 - loss: 0.0502 - val_accuracy: 0.9804 - val_loss: 0.0622\n",
      "Epoch 50/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9885 - loss: 0.0312 - val_accuracy: 0.9897 - val_loss: 0.0332\n",
      "Epoch 51/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 0.9914 - val_loss: 0.0285\n",
      "Epoch 52/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 0.9921 - val_loss: 0.0288\n",
      "Epoch 53/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.9921 - val_loss: 0.0282\n",
      "Epoch 54/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 0.9916 - val_loss: 0.0277\n",
      "Epoch 55/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 0.9919 - val_loss: 0.0261\n",
      "Epoch 56/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0061 - val_accuracy: 0.9484 - val_loss: 0.1590\n",
      "Epoch 57/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9842 - loss: 0.0492 - val_accuracy: 0.9876 - val_loss: 0.0401\n",
      "Epoch 58/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0100 - val_accuracy: 0.9835 - val_loss: 0.0615\n",
      "Epoch 59/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9967 - loss: 0.0100 - val_accuracy: 0.9912 - val_loss: 0.0259\n",
      "Epoch 60/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9966 - loss: 0.0100 - val_accuracy: 0.9909 - val_loss: 0.0300\n",
      "Epoch 61/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 0.9926 - val_loss: 0.0273\n",
      "Epoch 62/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 0.9847 - val_loss: 0.0528\n",
      "Epoch 63/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0154 - val_accuracy: 0.9878 - val_loss: 0.0467\n",
      "Epoch 64/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9970 - loss: 0.0122 - val_accuracy: 0.9907 - val_loss: 0.0384\n",
      "Epoch 65/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0101 - val_accuracy: 0.9892 - val_loss: 0.0366\n",
      "Epoch 66/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0056 - val_accuracy: 0.9871 - val_loss: 0.0484\n",
      "Epoch 67/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 0.9900 - val_loss: 0.0352\n",
      "Epoch 68/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9965 - loss: 0.0121 - val_accuracy: 0.9861 - val_loss: 0.0518\n",
      "Epoch 69/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9949 - loss: 0.0128 - val_accuracy: 0.9883 - val_loss: 0.0359\n",
      "Epoch 70/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0099 - val_accuracy: 0.9900 - val_loss: 0.0273\n",
      "Epoch 71/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9996 - loss: 0.0026 - val_accuracy: 0.9924 - val_loss: 0.0254\n",
      "Epoch 72/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 0.9931 - val_loss: 0.0229\n",
      "Epoch 73/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9924 - val_loss: 0.0241\n",
      "Epoch 74/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9993 - loss: 0.0018 - val_accuracy: 0.9926 - val_loss: 0.0273\n",
      "Epoch 75/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0143 - val_accuracy: 0.9699 - val_loss: 0.0991\n",
      "Epoch 76/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9880 - loss: 0.0346 - val_accuracy: 0.9854 - val_loss: 0.0499\n",
      "Epoch 77/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9965 - loss: 0.0102 - val_accuracy: 0.9919 - val_loss: 0.0360\n",
      "Epoch 78/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0052 - val_accuracy: 0.9933 - val_loss: 0.0268\n",
      "Epoch 79/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9940 - val_loss: 0.0257\n",
      "Epoch 80/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9990 - loss: 0.0022 - val_accuracy: 0.9938 - val_loss: 0.0248\n",
      "Epoch 81/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.9935 - val_loss: 0.0270\n",
      "Epoch 82/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9991 - loss: 0.0026 - val_accuracy: 0.9902 - val_loss: 0.0405\n",
      "Epoch 83/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0121 - val_accuracy: 0.9864 - val_loss: 0.0422\n",
      "Epoch 84/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9936 - loss: 0.0215 - val_accuracy: 0.9861 - val_loss: 0.0478\n",
      "Epoch 85/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0062 - val_accuracy: 0.9907 - val_loss: 0.0324\n",
      "Epoch 86/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0035 - val_accuracy: 0.9904 - val_loss: 0.0324\n",
      "Epoch 87/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9944 - loss: 0.0179 - val_accuracy: 0.9761 - val_loss: 0.0670\n",
      "Epoch 88/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9944 - loss: 0.0167 - val_accuracy: 0.9924 - val_loss: 0.0343\n",
      "Epoch 89/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9996 - loss: 0.0018 - val_accuracy: 0.9921 - val_loss: 0.0309\n",
      "Epoch 90/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9999 - loss: 9.9592e-04 - val_accuracy: 0.9926 - val_loss: 0.0294\n",
      "Epoch 91/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0015 - val_accuracy: 0.9943 - val_loss: 0.0284\n",
      "Epoch 92/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 0.9907 - val_loss: 0.0340\n",
      "Epoch 93/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0316 - val_accuracy: 0.9900 - val_loss: 0.0450\n",
      "Epoch 94/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0069 - val_accuracy: 0.9919 - val_loss: 0.0366\n",
      "Epoch 95/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.9943 - loss: 0.0174 - val_accuracy: 0.9931 - val_loss: 0.0300\n",
      "Epoch 96/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9928 - val_loss: 0.0362\n",
      "Epoch 97/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 0.9928 - val_loss: 0.0298\n",
      "Epoch 98/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0016 - val_accuracy: 0.9935 - val_loss: 0.0313\n",
      "Epoch 99/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9997 - loss: 9.5949e-04 - val_accuracy: 0.9938 - val_loss: 0.0297\n",
      "Epoch 100/100\n",
      "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9996 - loss: 9.5885e-04 - val_accuracy: 0.9926 - val_loss: 0.0352\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9922 - loss: 0.0374\n",
      "Final Model Accuracy: 99.26%\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping based on your dataset's sign names\n",
    "label_mapping = {\n",
    "    'اسمك ايه ؟': 0,\n",
    "    'اشاره': 1,\n",
    "    'الحمدلله': 2,\n",
    "    'السلام عليكم': 3,\n",
    "    'الصم': 4,\n",
    "    'اللغه العربيه': 5,\n",
    "    'ان شاء الله': 6,\n",
    "    'انا': 7,\n",
    "    'انت': 8,\n",
    "    'ايه ؟': 9,\n",
    "    'برنامج': 10,\n",
    "    'تخرج': 11,\n",
    "    'جميل': 12,\n",
    "    'دكتور': 13,\n",
    "    'شكرا': 14,\n",
    "    'طالب': 15,\n",
    "    'عامل ايه ؟': 16,\n",
    "    'فكرة': 17,\n",
    "    'في': 18,\n",
    "    'كلية حاسبات و معلومات': 19,\n",
    "    'مترجم': 20,\n",
    "    'مجتمع': 21,\n",
    "    'مساعده': 22,\n",
    "    'مشروع': 23,\n",
    "    'ناجح': 24,\n",
    "    'هدف': 25,\n",
    "    'و': 26,\n",
    "    'وعليكم السلام': 27,\n",
    "}\n",
    "output_numpy_dir = \"/kaggle/input/processed-arsl-dataset/processed_npy_arrays\"\n",
    "# Load data and train\n",
    "X_train, X_test, y_train, y_test = load_data(output_numpy_dir, label_mapping)\n",
    "model = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Final evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Final Model Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6770794,
     "sourceId": 10895057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
