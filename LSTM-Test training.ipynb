{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdSgpktzuByU",
        "outputId": "7495aa36-c1e5-4fae-9211-63ef6d45f708"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5SB5l11wpK9-",
        "outputId": "46e862a4-39a2-408e-d1dc-1a71b5597e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting arabic-reshaper\n",
            "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
            "Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: python-bidi, arabic-reshaper, sounddevice, mediapipe\n",
            "Successfully installed arabic-reshaper-3.0.0 mediapipe-0.10.21 python-bidi-0.6.6 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe tensorflow opencv-python arabic-reshaper python-bidi numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "adcM378y9keG"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Flatten\n",
        "from arabic_reshaper import reshape\n",
        "from bidi.algorithm import get_display\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Layer, LSTM, Dense, Permute, Multiply, Flatten\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVTFF0S37n2"
      },
      "source": [
        "1. Preprocessing with MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nQc89NAK0g0k"
      },
      "outputs": [],
      "source": [
        "mp_hands = mp.solutions.hands.Hands()\n",
        "# Function to extract pose and hand landmarks from a video\n",
        "def extract_landmarks(video_path):\n",
        "\n",
        "    global mp_hands\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    sequence = []   # List to hold frame data\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break    # Break if no frame\n",
        "\n",
        "        # Process frame\n",
        "        # Convert frame to RGB for MediaPipe processing\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the frame to get hand and pose landmarks\n",
        "        hand_results = mp_hands.process(frame_rgb)\n",
        "\n",
        "\n",
        "        # Initialize lists for hands landmarks (21 landmarks each)\n",
        "        left_hand = [[0,0,0]]*21\n",
        "        right_hand = [[0,0,0]]*21\n",
        "\n",
        "        # Extract hand landmarks\n",
        "        if hand_results.multi_hand_landmarks:\n",
        "            for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
        "                                      hand_results.multi_handedness):\n",
        "                if handedness.classification[0].label == \"Left\":\n",
        "                    left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "                else:\n",
        "                    right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "\n",
        "        # Flatten and combine all landmarks\n",
        "        frame_data = np.array(left_hand + right_hand).flatten()\n",
        "\n",
        "        # Only add frames with hands detected\n",
        "        if hand_results.multi_hand_landmarks:\n",
        "            sequence.append(frame_data)\n",
        "\n",
        "\n",
        "    cap.release()  # Release the video\n",
        "\n",
        "\n",
        "    if len(sequence) == 0:\n",
        "        print(f\"Warning: No hands detected in {video_path}\")\n",
        "    return np.array(sequence)\n",
        "\n",
        "    cap.release()  # Release the video\n",
        "    return np.array(sequence)  # Return the sequence of landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWRv5_-37n3"
      },
      "source": [
        "2. Data Conversion to Numpy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-cQWwgdU37n3"
      },
      "outputs": [],
      "source": [
        "def sequence_sampling(sequence, target_length):\n",
        "    # If sequence is shorter than target, pad with zeros\n",
        "    if len(sequence) <= target_length:\n",
        "        padded = np.zeros((target_length, sequence.shape[1]))\n",
        "        padded[:len(sequence)] = sequence\n",
        "        return padded\n",
        "\n",
        "    # If sequence is longer, sample frames evenly throughout the video\n",
        "    # np.linspace generates evenly spaced indices from start to end\n",
        "    indices = np.linspace(0, len(sequence)-1, target_length, dtype=int)\n",
        "    return sequence[indices]\n",
        "\n",
        "# Process videos for specific signs and save as numpy arrays. to split preprocessing\n",
        "def process_dataset(input_dir, output_dir, sign_list, seq_length=30):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dir: Path to dataset directory\n",
        "        output_dir: Base directory for processed output\n",
        "        sign_list: List of sign names to process( subset of the dataset )\n",
        "        seq_length: Target sequence length\n",
        "    \"\"\"\n",
        "    allowed_extensions = ['.mp4', '.avi', '.mov']  # List of video file extensions\n",
        "\n",
        "    # Loop through each requested sign\n",
        "    for sign_name in sign_list:\n",
        "        sign_path = os.path.join(input_dir, sign_name)\n",
        "        if not os.path.isdir(sign_path):\n",
        "            print(f\"Warning: Sign '{sign_name}' not found in {input_dir}\")\n",
        "            continue\n",
        "\n",
        "        # Create output directory for this sign\n",
        "        sign_output_dir = os.path.join(output_dir, sign_name)\n",
        "        os.makedirs(sign_output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Processing sign: {sign_name}\")\n",
        "\n",
        "        # Loop through each signer folder\n",
        "        for signer_name in os.listdir(sign_path):\n",
        "            signer_path = os.path.join(sign_path, signer_name)\n",
        "            if not os.path.isdir(signer_path):\n",
        "                continue\n",
        "\n",
        "            # Loop through each video file\n",
        "            for video_file in os.listdir(signer_path):\n",
        "                # Skip non-video files\n",
        "                if not any(video_file.lower().endswith(ext) for ext in allowed_extensions):\n",
        "                    continue\n",
        "\n",
        "                video_path = os.path.join(signer_path, video_file)\n",
        "\n",
        "                # Extract landmarks from video\n",
        "                sequence = extract_landmarks(video_path)\n",
        "\n",
        "                if len(sequence) == 0:  # Skip videos with no hands detected\n",
        "                    print(f\"  Skipping {video_file} - no hands detected\")\n",
        "                    continue\n",
        "\n",
        "                # Update features count for hands only (21 landmarks per hand, 3 coordinates per landmark)\n",
        "                num_features = 42 * 3  # 2 hands × 21 landmarks × 3 coordinates\n",
        "\n",
        "                # Use smart sequence sampling instead of simple truncation/padding\n",
        "                padded_sequence = sequence_sampling(sequence, seq_length)\n",
        "\n",
        "                # Save with signer info in filename\n",
        "                base_name = os.path.splitext(video_file)[0]\n",
        "                numpy_filename = f\"{signer_name}_{base_name}.npy\"\n",
        "                np.save(os.path.join(sign_output_dir, numpy_filename), padded_sequence)\n",
        "\n",
        "    print(f\"Processing complete. Output saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "geKXPhiftrhh"
      },
      "outputs": [],
      "source": [
        "# !ls V1/mini_dataset/\n",
        "# lists for different notebooks\n",
        "# sign_list = [\n",
        "#     \"اسمك ايه ؟\", \"اشاره\", \"الحمدلله\", \"السلام عليكم\", \"الصم\", \"اللغه العربيه\",\n",
        "#     \"ان شاء الله\", \"انا\", \"انت\", \"ايه ؟\", \"برنامج\", \"تخرج\", \"جميل\", \"دكتور\",\n",
        "#     \"شكرا\", \"طالب\", \"عامل ايه ؟\", \"فكرة\", \"في\", \"كلية حاسبات و معلومات\",\n",
        "#     \"مترجم\", \"مجتمع\", \"مساعده\", \"مشروع\", \"ناجح\", \"هدف\", \"و\", \"وعليكم السلام\"\n",
        "# ]\n",
        "\n",
        "list1 = ['اسمك ايه ؟', 'اشاره', 'الحمدلله', 'السلام عليكم', 'الصم', 'اللغه العربيه', 'ان شاء الله']\n",
        "# list2 = ['انا', 'انت', 'ايه ؟', 'برنامج', 'تخرج', 'جميل', 'دكتور']\n",
        "# list3 = ['شكرا', 'طالب', 'عامل ايه ؟', 'فكرة', 'في', 'كلية حاسبات و معلومات', 'مترجم']\n",
        "# list4 = ['مجتمع', 'مساعده', 'مشروع', 'ناجح', 'هدف', 'و', 'وعليكم السلام']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fennVKM4uAaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d62c547-2efd-4f70-b9ca-edc6b0c6ec14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sign: اسمك ايه ؟\n"
          ]
        }
      ],
      "source": [
        "# Set the paths\n",
        "input_video_dir = r\"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/Videos\"  # Raw videos (28 subfolders)\n",
        "output_numpy_dir = r\"/content/drive/MyDrive/SignComm_Dataset/ArSL_Dataset/processed_videos\"  # Processed numpy data\n",
        "# model_save_path = \"/model.h5\"     # Trained model path\n",
        "\n",
        "# to process all videos\n",
        "#should probably add something to the function to show progress\n",
        "process_dataset(input_dir= input_video_dir, output_dir = output_numpy_dir, sign_list = list1, seq_length=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDOlaw2uPIO2"
      },
      "source": [
        "3. Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mtV0Z6cPIO2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to load data from numpy arrays and prepare for model training\n",
        "def load_data(numpy_dir, label_mapping, test_size=0.15, val_size=0.15, random_state=42):\n",
        "    X = []  # List to hold feature data\n",
        "    y = []  # List to hold label data\n",
        "\n",
        "    # Loop through each sign directory\n",
        "    for sign_name in os.listdir(numpy_dir):\n",
        "        sign_dir = os.path.join(numpy_dir, sign_name)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(sign_dir):\n",
        "            continue\n",
        "\n",
        "        # Get class index for this sign\n",
        "        class_idx = label_mapping[sign_name]\n",
        "\n",
        "        # Process all numpy files in this sign directory\n",
        "        for file in os.listdir(sign_dir):\n",
        "            if not file.endswith(\".npy\") or \"Mohamed\" in file:\n",
        "                continue\n",
        "\n",
        "            # Load numpy array from the file\n",
        "            data_path = os.path.join(sign_dir, file)\n",
        "            data = np.load(data_path)\n",
        "\n",
        "            # Append data and label\n",
        "            X.append(data)\n",
        "            y.append(class_idx)\n",
        "\n",
        "    X = np.array(X)  # Convert feature list to numpy array\n",
        "    num_classes = len(label_mapping)\n",
        "    y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "\n",
        "    # First split: separate test set\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Second split: separate validation set from remaining data\n",
        "    # Adjust validation size to account for the reduced dataset\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktNi97CS37n3"
      },
      "source": [
        "4. LSTM Model with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZT57gZPIO2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define custom temporal attention layer\n",
        "class TemporalAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize attention weight\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='normal')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Calculate attention weights\n",
        "        e = tf.tanh(tf.matmul(x, self.W))\n",
        "        a = tf.nn.softmax(e, axis=1)\n",
        "        # Apply attention to the input sequence\n",
        "        output = x * a\n",
        "        # Aggregate the attentionaly weighted features over the sequence\n",
        "        return tf.reduce_sum(output, axis=1)\n",
        "\n",
        "# Function to build the LSTM model with attention\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Bi-directional LSTM layers with return sequences\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(inputs)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "    # Temporal Attention Layer\n",
        "    attention = TemporalAttention()(x)\n",
        "\n",
        "    # Classification using dense layer\n",
        "    outputs = Dense(num_classes, activation='softmax')(attention)\n",
        "\n",
        "    # Create model object\n",
        "    model = Model(inputs, outputs)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052pHi3n37n4"
      },
      "source": [
        "5. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe_d1acE37n4"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate the model\n",
        "def train_model(X_train, X_val, X_test, y_train, y_val, y_test, num_classes, epochs=5, batch_size=8):\n",
        "    # Build the model using the specified input shape and number of classes\n",
        "    model = build_model(X_train.shape[1:], num_classes)\n",
        "\n",
        "    # Create output directory for model checkpoints\n",
        "    os.makedirs('model_checkpoints', exist_ok=True)\n",
        "\n",
        "    # Define callbacks for saving the best model and learning rate reduction\n",
        "    callbacks = [\n",
        "        # Save the best model based on validation accuracy\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'model_checkpoints/best_model.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        # Reduce learning rate when validation accuracy plateaus\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model using validation data for monitoring\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),  # Use validation set during training\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWm6NolVPIO2"
      },
      "outputs": [],
      "source": [
        "# Create label mapping based on your dataset's sign names\n",
        "label_mapping_main = {\n",
        "    'اسمك ايه ؟': 0,\n",
        "    'اشاره': 1,\n",
        "    'الحمدلله': 2,\n",
        "    'السلام عليكم': 3,\n",
        "    'اللغه العربيه': 4,\n",
        "    'ان شاء الله': 5,\n",
        "    'انا': 6,\n",
        "    'انت': 7,\n",
        "    'ايه ؟': 8,\n",
        "    'برنامج': 9,\n",
        "    'تخرج': 10,\n",
        "    'جميل': 11,\n",
        "    'دكتور': 12,\n",
        "    'شكرا': 13,\n",
        "    'الصم': 14,\n",
        "    'طالب': 15,\n",
        "    'عامل ايه ؟': 16,\n",
        "    'فكرة': 17,\n",
        "    'في': 18,\n",
        "    'كلية حاسبات و معلومات': 19,\n",
        "    'مترجم': 20,\n",
        "    'مجتمع': 21,\n",
        "    'مساعده': 22,\n",
        "    'مشروع': 23,\n",
        "    'ناجح': 24,\n",
        "    'هدف': 25,\n",
        "    'وعليكم السلام': 26,\n",
        "    'و': 27,\n",
        "}\n",
        "\n",
        "# Create label mapping using only the uncommented labels\n",
        "label_mapping = {\n",
        "    'اسمك ايه ؟': 0,\n",
        "    'اشاره': 1,\n",
        "    'الحمدلله': 2,\n",
        "    'السلام عليكم': 3,\n",
        "    'اللغه العربيه': 4,\n",
        "    'ان شاء الله': 5,\n",
        "    'الصم': 6,\n",
        "}\n",
        "\n",
        "processed_data_dir = r\"E:\\Current Semester\\GP\\ArSL_Model\\V1\\mini_output\"  # Update this path\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = load_data(\n",
        "    numpy_dir=processed_data_dir,\n",
        "    label_mapping=label_mapping,\n",
        "    test_size=0.15,\n",
        "    val_size=0.15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Get number of classes from label mapping\n",
        "num_classes = len(label_mapping)\n",
        "print(f\"Training with {num_classes} classes\")\n",
        "\n",
        "\n",
        "model, history = train_model(\n",
        "    X_train, X_val, X_test,\n",
        "    y_train, y_val, y_test,\n",
        "    num_classes=num_classes,\n",
        "    epochs=10,  # Increase for better results\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "# Final evaluation on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Final Model Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "# Save the final model\n",
        "model.save('arsl_recognition_model_2.keras')\n",
        "print(\"Model saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfVkZdGTtrhj"
      },
      "outputs": [],
      "source": [
        "# Visualize training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVsH0wlDPIO2"
      },
      "source": [
        "6. Arabic Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGu5a6nTPIO3"
      },
      "outputs": [],
      "source": [
        "# Function to display arabic text on frame\n",
        "def display_arabic_text(frame, text):\n",
        "    reshaped_text = reshape(text)  # Reshape text for Arabic display\n",
        "    bidi_text = get_display(reshaped_text)  # Get display text for correct display order\n",
        "    cv2.putText(frame, bidi_text, (50,100),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "               (0,255,0), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROJSevnFPIO3"
      },
      "outputs": [],
      "source": [
        "# Function to get the Arabic label from the index\n",
        "def get_arabic_label(index):\n",
        "    arabic_labels = [ \"اسمك ايه ؟\", \"اشاره\", \"الحمدلله\",\"السلام عليكم\",\"اللغه العربيه\",\"ان شاء الله\",\"انا\",\"انت\",\"ايه ؟\",\"برنامج\",\"تخرج\",\n",
        "    \"جميل\",\"دكتور\",\"شكرا\",\"الصم\",\"طالب\",\"عامل ايه ؟\",\"فكرة\",\"في\",\"كلية حاسبات و معلومات\",\"مترجم\",\"مجتمع\",\"مساعده\",\"مشروع\",\"ناجح\",\"هدف\",\"وعليكم السلام\",\"و\"]\n",
        "    return arabic_labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYYijCcz37n4"
      },
      "source": [
        "7. Real-Time Translation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for realtime translation\n",
        "\n",
        "# Initialize MediaPipe hand solution outside the function\n",
        "mp_hands = mp.solutions.hands.Hands()\n",
        "# mp_pose = mp.solutions.pose.Pose()  # Comment out or remove this line\n",
        "\n",
        "# Function to extract landmarks from a single frame\n",
        "def extract_landmarks_single(frame):\n",
        "    # Use the global MediaPipe hand solution\n",
        "    global mp_hands\n",
        "\n",
        "    # Convert frame to RGB for MediaPipe\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process hand landmarks\n",
        "    hand_results = mp_hands.process(frame_rgb)\n",
        "\n",
        "    # Hand landmarks\n",
        "    left_hand = [[0,0,0]]*21\n",
        "    right_hand = [[0,0,0]]*21\n",
        "\n",
        "    if hand_results.multi_hand_landmarks:\n",
        "        for hand, handedness in zip(hand_results.multi_hand_landmarks,\n",
        "                                  hand_results.multi_handedness):\n",
        "            if handedness.classification[0].label == \"Left\":\n",
        "                left_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "            else:\n",
        "                right_hand = [[lmk.x, lmk.y, lmk.z] for lmk in hand.landmark]\n",
        "\n",
        "    # Flatten and return\n",
        "    frame_data = np.array(left_hand + right_hand).flatten()\n",
        "    return frame_data if (np.any(left_hand) or np.any(right_hand)) else None"
      ],
      "metadata": {
        "id": "6LX7rEjNt0pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVFngfPH37n4"
      },
      "outputs": [],
      "source": [
        "# Function for real-time translation\n",
        "def real_time_translation(model, seq_length=30):\n",
        "    cap = cv2.VideoCapture(0)  # Open default camera\n",
        "    buffer = []  # Initialize frame buffer\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()   # Read frame from the camera\n",
        "        if not ret: break   # Break if no frame is read\n",
        "\n",
        "        # Process the frame to get hand and pose landmarks\n",
        "        processed_frame = extract_landmarks_single(frame)\n",
        "\n",
        "        if processed_frame is None:\n",
        "            # Display a text to show hands if not detected\n",
        "            cv2.putText(frame, \"Show Hands\", (50,50),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                      (0,0,255), 2)\n",
        "\n",
        "        # Append the frame to the buffer if landmarks are detected\n",
        "        else:  # Has hands\n",
        "            buffer.append(processed_frame) # Append the processed frame to buffer\n",
        "            buffer = [f for f in buffer if f is not None][-seq_length:]  # Keep only the most recent frames and filter out any None\n",
        "\n",
        "\n",
        "            if len(buffer) == seq_length:\n",
        "                # Make a prediction using the model\n",
        "                prediction = model.predict(np.array([buffer]))\n",
        "                arabic_word = get_arabic_label(np.argmax(prediction)) # Get the predicted word\n",
        "                display_arabic_text(frame, arabic_word) # Display it on the frame\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow('Translation', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit if 'q' is pressed\n",
        "            break\n",
        "\n",
        "    cap.release()  # Release the camera\n",
        "    cv2.destroyAllWindows()  # Close all windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ayz8s237n4"
      },
      "source": [
        "8. Save/Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Uo1_ag37n4"
      },
      "outputs": [],
      "source": [
        "# Function to save the model\n",
        "def save_model(model, path):\n",
        "    model.save(path)\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(path):\n",
        "    return tf.keras.models.load_model(\n",
        "        path,\n",
        "        custom_objects={'TemporalAttention': TemporalAttention}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxdfI4i_PIO3"
      },
      "outputs": [],
      "source": [
        "model = load_model(model_save_path)\n",
        "real_time_translation(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}